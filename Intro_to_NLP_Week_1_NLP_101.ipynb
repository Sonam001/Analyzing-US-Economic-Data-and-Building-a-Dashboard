{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to NLP: Week 1 - NLP 101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sonam001/Analyzing-US-Economic-Data-and-Building-a-Dashboard/blob/master/Intro_to_NLP_Week_1_NLP_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob5fTzLcJSnj",
        "colab_type": "text"
      },
      "source": [
        "# Intro to NLP (Natural Language Processing): Week 1 - NLP 101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa8jvj1LLEL9",
        "colab_type": "text"
      },
      "source": [
        "## EDA (Exploratory Data Analysis)\n",
        "\n",
        "\"Exploratory Data Analysis is the process of exploring data, generating insights, testing hypotheses, checking assumptions and revealing underlying hidden patterns in the data\".\n",
        "\n",
        "Through these goals, we can get a basic description of the data, visualize it, identify pattern in it, identify potential challenges of using the data, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOqsBpPeRDOY",
        "colab_type": "text"
      },
      "source": [
        "A website with a wealth of datasets is the UCI Machine Learning repository (https://archive.ics.uci.edu/ml/index.php). One of these is the SMS Spam Collection Data Set, which is a public collection of SMS labeled messages that have been collected for mobile phone spam search. \n",
        "\n",
        "Note: refer to this website for more information on the dataset: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAiKbrhvTvBD",
        "colab_type": "text"
      },
      "source": [
        "The dataset (zip file) can be downloaded from this URL -- https://archive.ics.uci.edu/ml/machine-learning-databases/00228/. \n",
        "\n",
        "Steps to access, download, and use the dataset:\n",
        "\n",
        "\n",
        "1.   Access the above link, click on \"smsspamcollection.zip\" to download the dataset. \n",
        "2.   Once downloaded, click on the zipped folder to compress it.\n",
        "3.   Save this folder to a location on your computer that you will remember, such as \"Documents\".\n",
        "4.   Navigate to the file system in the pane to the left, and upload the dataset folder here. \n",
        "5.   We will next access this dataset to load it in and start EDA and pre-processing! It is located at: '/content/SMSSpamCollection'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawQEVyTLHPR",
        "colab_type": "text"
      },
      "source": [
        "Before we start exploring the data, we must first load in the data. You may be familiar with this, but the Pandas library actually makes this a simple task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq7zWODDLXQz",
        "colab_type": "text"
      },
      "source": [
        "First, you import the Pandas package as pd. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExCELPIgLXet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RtrI7ECLdI5",
        "colab_type": "text"
      },
      "source": [
        "And then, we use the read_csv() function for loading in the data. We pass into this function the URL in which the data can be found. To make it easier to work with the data in the future, we will name it by assigning it to a variable (digits in this case). \n",
        "\n",
        "Note: we use \"header = None\" here so that the first row of our data will not be interpreted as the column names of the data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlCiEGN0Lddx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sms = pd.read_table('/content/SMSSpamCollection', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMkq2sXeL41o",
        "colab_type": "text"
      },
      "source": [
        "In order to see what data we loaded in, we can simply do this below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "654CaLnWL5Jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "4a81151b-a6e7-4e9b-8ab5-d26d902daa48"
      },
      "source": [
        "print(sms)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         0                                                  1\n",
            "0      ham  Go until jurong point, crazy.. Available only ...\n",
            "1      ham                      Ok lar... Joking wif u oni...\n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      ham  U dun say so early hor... U c already then say...\n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
            "...    ...                                                ...\n",
            "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
            "5568   ham               Will ü b going to esplanade fr home?\n",
            "5569   ham  Pity, * was in mood for that. So...any other s...\n",
            "5570   ham  The guy did some bitching but I acted like i'd...\n",
            "5571   ham                         Rofl. Its true to its name\n",
            "\n",
            "[5572 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrs6YChqCsg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "dd6bfed6-08dc-4e83-9b12-7aaf426d1e42"
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0                                                  1\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEDEHD0oJXzD",
        "colab_type": "text"
      },
      "source": [
        "### Describe the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QISsuHR5Mlj_",
        "colab_type": "text"
      },
      "source": [
        "One of the most elementary steps to do this is by getting a basic description of your data. A basic description of your data is indeed a very broad term: you can interpret it as a quick and dirty way to get some information on your data, as a way of getting some simple, easy-to-understand information on your data, to get a basic feel for your data, etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7GI-NSJMypB",
        "colab_type": "text"
      },
      "source": [
        "To begin, we can use the describe() function to obtain various summary statistics that exclude NaN values. We can refer back to our digits data and understand it a bit better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpzktpYPLIei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "c0da75ba-8379-4871-da72-38838296d371"
      },
      "source": [
        "sms.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5572</td>\n",
              "      <td>5572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>5169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>ham</td>\n",
              "      <td>Sorry, I'll call later</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>4825</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0                       1\n",
              "count   5572                    5572\n",
              "unique     2                    5169\n",
              "top      ham  Sorry, I'll call later\n",
              "freq    4825                      30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucNsl21IM_W4",
        "colab_type": "text"
      },
      "source": [
        "You see that this function returns the count, mean, standard deviation, minimum and maximum values and the quantiles of the data. Note that, of course, there are many packages available in Python that can give you those statistics, including Pandas itself. Using this function is just one of the ways to get this information. You can use these descriptive statistics to begin to assess the quality of your data. Then you’ll be able to decide whether you need to correct, discard or deal with the data in anohter way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyubgQxRK7aU",
        "colab_type": "text"
      },
      "source": [
        "### Data Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoBb6vu9NLbz",
        "colab_type": "text"
      },
      "source": [
        "Here, we have a collection of text data known as a corpus. Specifically, there are 5,572 SMS messages written in English, serving as training examples. The first column is the target variable containing the class labels, which tells us if the message is spam or ham (aka not spam). The second column is the SMS message itself, stored as a string."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM6wT7MWWLdG",
        "colab_type": "text"
      },
      "source": [
        "Since the target variable contains discrete values, this is a **classification** task. Let's start by placing the target variable in its own table and checking out how the two classes are distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL77rJf7NfVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "10da2e2b-8738-4636-b60b-1a5214cfb63e"
      },
      "source": [
        "y = sms[0]\n",
        "y.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPrTvN3mNNOq",
        "colab_type": "text"
      },
      "source": [
        "It looks like there are far fewer training examples for spam than ham—we'll take this imbalance into account in the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8CwsiEsWdD-",
        "colab_type": "text"
      },
      "source": [
        "In addition, we need to encode the class labels in the target variable as numbers to ensure compatibility with some models in Scikit-learn. Because we have binary classes, let's use LabelEncoder and set 'spam' = 1 and 'ham' = 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08MzZah8XI4e",
        "colab_type": "text"
      },
      "source": [
        "LabelEncoder is a function of scikit learn's preprocessing capabilities, which helps to encode target labsls with values between 0 and the (# of classes) - 1. \n",
        "\n",
        "Note: Refer to this website for more information (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJxyWauaXInG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWm_nsPzLFlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "y_enc = le.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtX_JCKBXjZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12658487-cfbe-4219-8042-9f3348a96895"
      },
      "source": [
        "y_enc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxdAQXrjWkr1",
        "colab_type": "text"
      },
      "source": [
        "Next, we place the SMS message data into its own table. We must convert this corpus into useful numerical features so we can train this classifier and this is where NLP works its magic!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PYEcisJW8zG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text = sms[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6UsVe0OW69q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "7c076c9f-b0f0-4f67-8057-fd3b87d9bc14"
      },
      "source": [
        "raw_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Go until jurong point, crazy.. Available only ...\n",
              "1                           Ok lar... Joking wif u oni...\n",
              "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3       U dun say so early hor... U c already then say...\n",
              "4       Nah I don't think he goes to usf, he lives aro...\n",
              "                              ...                        \n",
              "5567    This is the 2nd time we have tried 2 contact u...\n",
              "5568                 Will ü b going to esplanade fr home?\n",
              "5569    Pity, * was in mood for that. So...any other s...\n",
              "5570    The guy did some bitching but I acted like i'd...\n",
              "5571                           Rofl. Its true to its name\n",
              "Name: 1, Length: 5572, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIdoXqLaPY7O",
        "colab_type": "text"
      },
      "source": [
        "Another important part of any dataset is missing values. When this happens, the dataset can lose expressiveness, which may lead to weak or at times biased analyses. Practically, this means that when you’re missing values for certain features, the chances of your classification or predictions for the data being off only increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwXB_McsPjqC",
        "colab_type": "text"
      },
      "source": [
        "To identify the rows that contain missing values, you can use isnull(). In the result that you’ll get back, you’ll see True or False appearing in each cell: True will indicate that the value contained within the cell is a missing value, False means that the cell contains a ‘normal’ value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvdbliy1PkCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "822563e0-cdcf-4d75-eb4a-7bdcafd8fcc6"
      },
      "source": [
        "pd.isnull(sms)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0      1\n",
              "0     False  False\n",
              "1     False  False\n",
              "2     False  False\n",
              "3     False  False\n",
              "4     False  False\n",
              "...     ...    ...\n",
              "5567  False  False\n",
              "5568  False  False\n",
              "5569  False  False\n",
              "5570  False  False\n",
              "5571  False  False\n",
              "\n",
              "[5572 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHLGVRvBPuox",
        "colab_type": "text"
      },
      "source": [
        "In this case, you see that the data is quite complete: there are no missing values and you're lucky! But this is definitely not always the case for datasets out there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZgFXvPeK731",
        "colab_type": "text"
      },
      "source": [
        "### Basic Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLqnQ-KuQEu7",
        "colab_type": "text"
      },
      "source": [
        "Data visualization can help with identifying patterns in the data. The Python libraries Seaborn and Matplotlib are easy and quick ways to do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smz8QSJBLEJg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "45a60e26-7362-411d-8164-f6b15234adf4"
      },
      "source": [
        "import matplotlib as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52s9-Oc1XvgX",
        "colab_type": "text"
      },
      "source": [
        "There are a couple basic visualizations we can do. The first is displaying the length of all the dataset instances. To do this, we must first label the columns with their appropriate titles and add a column to the dataset that contains the length. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD-0ZhgOqPZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sms.columns=['label', 'msg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeEVeKxZq2c3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "55944fa6-c60a-44e0-8c3c-64cdb17db3ba"
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                msg\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f02B7Y6Y--F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sms[\"length\"] = sms[\"msg\"].apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZvGSe3Kq9Q9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "17faa196-a10d-467e-b5e4-26c09fba6bf5"
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                msg  length\n",
              "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
              "1   ham                      Ok lar... Joking wif u oni...      29\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
              "3   ham  U dun say so early hor... U c already then say...      49\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tGjoINoXvJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5a686ef6-e677-40e5-b3f0-aebaa31f8603"
      },
      "source": [
        "sns.distplot(sms[\"length\"], kde=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff0f5e04ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X0lTCL0KFqk",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "- https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/\n",
        "- https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python\n",
        "- https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Sal7tUXVBj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZEmaQyrLIzI",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwnXlxfWrUoR",
        "colab_type": "text"
      },
      "source": [
        "There are many feature engineering strategies for transforming text data into features. Some involve assigning each unique word-like term to a feature and counting the number of occurrences per training example. However, if we were to perform this strategy right now, we'd end up with an absurd number of features, a result of the myriad possible terms. The classifier would take too long to train and likely overfit. As a result, each NLP problem requires a tailored approach to determine which terms are relevant and meaningful, and this is where we begin our pre-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dSlUTVNJjAU",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Noise Cleaning - spacing, special characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eTvf_eKrrEw",
        "colab_type": "text"
      },
      "source": [
        "Let'a take a small step back and examine a few random examples of SMS messages from our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGeQt7LCsuQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "outputId": "230a3a29-cc67-47a2-a032-c0473431f171"
      },
      "source": [
        "\n",
        "\n",
        "sms['lower'] = sms['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>wordnet_pos</th>\n",
              "      <th>lemmatized</th>\n",
              "      <th>stopwords_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[(Go, NNP), (until, IN), (jurong, JJ), (point,...</td>\n",
              "      <td>[(Go, n), (until, n), (jurong, a), (point,, n)...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>[go, jurong, point, crazy.., available, bugis,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...</td>\n",
              "      <td>[(Ok, n), (lar..., v), (Joking, n), (wif, n), ...</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[(Free, JJ), (entry, NN), (in, IN), (2, CD), (...</td>\n",
              "      <td>[(Free, a), (entry, n), (in, n), (2, n), (a, n...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[(you, PRP), (dun, VBP), (say, VB), (so, RB), ...</td>\n",
              "      <td>[(you, n), (dun, v), (say, v), (so, r), (early...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>[u, dun, say, early, hor, ..., u, c, already, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, he...</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[(Nah, NNP), (I, PRP), (do not, VBP), (think, ...</td>\n",
              "      <td>[(Nah, n), (I, n), (do not, v), (think, v), (h...</td>\n",
              "      <td>[Nah, I, do not, think, he, go, to, usf,, he, ...</td>\n",
              "      <td>[nah, n't, think, goes, usf, lives, around, th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                  stopwords_removed\n",
              "0   ham  ...  [go, jurong, point, crazy.., available, bugis,...\n",
              "1   ham  ...           [ok, lar, ..., joking, wif, u, oni, ...]\n",
              "2  spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
              "3   ham  ...  [u, dun, say, early, hor, ..., u, c, already, ...\n",
              "4   ham  ...  [nah, n't, think, goes, usf, lives, around, th...\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZAoKyvnrxmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "d710e28a-d943-4062-ac55-bab2a8355759"
      },
      "source": [
        "sms.sample(frac=0.05)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4288</th>\n",
              "      <td>ham</td>\n",
              "      <td>I wud never mind if u dont miss me or if u don...</td>\n",
              "      <td>139</td>\n",
              "      <td>[I, wud, never, mind, if, u, dont, miss, me, o...</td>\n",
              "      <td>[i, wud, never, mind, if, u, dont, miss, me, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4243</th>\n",
              "      <td>spam</td>\n",
              "      <td>Show ur colours! Euro 2004 2-4-1 Offer! Get an...</td>\n",
              "      <td>135</td>\n",
              "      <td>[Show, ur, colours, !, Euro, 2004, 2-4-1, Offe...</td>\n",
              "      <td>[show, ur, colours, !, euro, 2004, 2-4-1, offe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2509</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok...</td>\n",
              "      <td>5</td>\n",
              "      <td>[Ok, ...]</td>\n",
              "      <td>[ok, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>ham</td>\n",
              "      <td>You were supposed to wake ME up &amp;gt;:(</td>\n",
              "      <td>38</td>\n",
              "      <td>[You, were, supposed, to, wake, ME, up, &amp;, gt,...</td>\n",
              "      <td>[you, were, supposed, to, wake, me, up, &amp;, gt,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>ham</td>\n",
              "      <td>What you doing?how are you?</td>\n",
              "      <td>27</td>\n",
              "      <td>[What, you, doing, ?, how, are, you, ?]</td>\n",
              "      <td>[what, you, doing, ?, how, are, you, ?]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593</th>\n",
              "      <td>spam</td>\n",
              "      <td>PRIVATE! Your 2003 Account Statement for 07753...</td>\n",
              "      <td>148</td>\n",
              "      <td>[PRIVATE, !, Your, 2003, Account, Statement, f...</td>\n",
              "      <td>[private, !, your, 2003, account, statement, f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2518</th>\n",
              "      <td>ham</td>\n",
              "      <td>Sorry, I'll call later</td>\n",
              "      <td>22</td>\n",
              "      <td>[Sorry, ,, I, 'll, call, later]</td>\n",
              "      <td>[sorry, ,, i, 'll, call, later]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4671</th>\n",
              "      <td>ham</td>\n",
              "      <td>I didnt get ur full msg..sometext is missing, ...</td>\n",
              "      <td>59</td>\n",
              "      <td>[I, didnt, get, ur, full, msg..sometext, is, m...</td>\n",
              "      <td>[i, didnt, get, ur, full, msg..sometext, is, m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5228</th>\n",
              "      <td>spam</td>\n",
              "      <td>PRIVATE! Your 2003 Account Statement for &lt;fone...</td>\n",
              "      <td>146</td>\n",
              "      <td>[PRIVATE, !, Your, 2003, Account, Statement, f...</td>\n",
              "      <td>[private, !, your, 2003, account, statement, f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4315</th>\n",
              "      <td>ham</td>\n",
              "      <td>Is ur changes 2 da report big? Cos i've alread...</td>\n",
              "      <td>82</td>\n",
              "      <td>[Is, ur, changes, 2, da, report, big, ?, Cos, ...</td>\n",
              "      <td>[is, ur, changes, 2, da, report, big, ?, cos, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>279 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label  ...                                              lower\n",
              "4288   ham  ...  [i, wud, never, mind, if, u, dont, miss, me, o...\n",
              "4243  spam  ...  [show, ur, colours, !, euro, 2004, 2-4-1, offe...\n",
              "2509   ham  ...                                          [ok, ...]\n",
              "442    ham  ...  [you, were, supposed, to, wake, me, up, &, gt,...\n",
              "253    ham  ...            [what, you, doing, ?, how, are, you, ?]\n",
              "...    ...  ...                                                ...\n",
              "593   spam  ...  [private, !, your, 2003, account, statement, f...\n",
              "2518   ham  ...                    [sorry, ,, i, 'll, call, later]\n",
              "4671   ham  ...  [i, didnt, get, ur, full, msg..sometext, is, m...\n",
              "5228  spam  ...  [private, !, your, 2003, account, statement, f...\n",
              "4315   ham  ...  [is, ur, changes, 2, da, report, big, ?, cos, ...\n",
              "\n",
              "[279 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvvhKUHGr_Jt",
        "colab_type": "text"
      },
      "source": [
        "Clearly there's a lot going on here: digits, gratuitous whitespace, and all varieties of punctuation. Some terms are randomly capitalized, others are in all-caps. Since these terms might show up in any one of the training examples in countless forms, we need a way to ensure each training example is on equal footing via a preprocessing step called normalization. This form of noise cleaning takes care of spacing and any special characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpt8o7Evlvj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-9loSCBsnKL",
        "colab_type": "text"
      },
      "source": [
        "Transforming all words to lowercase is also a very common pre-processing step. In this case, we will once again append a new column named “lower” to the dataframe which will transform all the tokenized words into lowercase. However, because we have to iterate over multiple words we will use a simple for-loop within a lambda function to apply the “lower” function to each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAovmw7AsxCt",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll remove all punctuation since they serve little value once we begin to analyze our data. Continuing the previous pattern, we will create a new column which has the punctuation removed. We will again utilize a for-loop within a lambda function to iterate over the tokens but this time using an IF condition to only output alpha characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmK_8wVts0ql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "8f27e4a6-2090-4b95-89ca-ff56dde5542f"
      },
      "source": [
        "import string\n",
        "punc = string.punctuation\n",
        "sms['no_punc'] = sms['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                            no_punc\n",
              "0   ham  ...  [go, until, jurong, point, crazy.., available,...\n",
              "1   ham  ...           [ok, lar, ..., joking, wif, u, oni, ...]\n",
              "2  spam  ...  [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
              "3   ham  ...  [u, dun, say, so, early, hor, ..., u, c, alrea...\n",
              "4   ham  ...  [nah, i, do, n't, think, he, goes, to, usf, he...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fD7Vjr8JlEI",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF5N0vquuMV1",
        "colab_type": "text"
      },
      "source": [
        "In this step, we construct the features. We will begin by breaking apart the corpus into a vocabulary of unique terms, and this is called tokanization. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_W-ztrPuUcm",
        "colab_type": "text"
      },
      "source": [
        "We can tokenize individual terms and generate what's called a bag of words model. You may notice this model has a glaring pitfall: it fails to capture the innate structure of human language. We can also tokenize using nltk, which is the leading platform for building Python programs to work with human language data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw_4LrBeuOAn",
        "colab_type": "text"
      },
      "source": [
        "We will begin my installing and importing nltk, so we can use it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCm7QGpCTnld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "12f27885-03b4-4910-fb28-e854bbfc7a7e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkE5LU-WvkAK",
        "colab_type": "text"
      },
      "source": [
        "First, we will take a basic sentence to demonstrate this tokenization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVAOYdROv3u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Hi, I would like to tokenize this sentence.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcfmSnNfTxBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f0d415d-5f2f-4319-c6ac-715c027d8389"
      },
      "source": [
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi', ',', 'I', 'would', 'like', 'to', 'tokenize', 'this', 'sentence', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP2etJNC-a-E",
        "colab_type": "text"
      },
      "source": [
        "Now, we can apply the tokenizer to our dataset. We will apply NLTK.word_tokenize() function to the “rating_description_str” column and create a new column named “tokenized”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXKZEWZG-gII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "412708e3-1af0-4234-bbd3-adc560d92334"
      },
      "source": [
        "sms['tokenized'] = sms['msg'].apply(word_tokenize)\n",
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                          tokenized\n",
              "0   ham  ...  [Go, until, jurong, point, ,, crazy.., Availab...\n",
              "1   ham  ...           [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
              "2  spam  ...  [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
              "3   ham  ...  [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
              "4   ham  ...  [Nah, I, do, n't, think, he, goes, to, usf, ,,...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnYvZAOj23RF",
        "colab_type": "text"
      },
      "source": [
        "Reference: https://www.guru99.com/tokenize-words-sentences-nltk.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr24n8hFJm3I",
        "colab_type": "text"
      },
      "source": [
        " ### Step 3: Spell Checking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsKljmGpv8E6",
        "colab_type": "text"
      },
      "source": [
        "For spell checking, we will use Microsoft's TextBlob, which is a simple spelling correction mechanism. You can install this using the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJZmd45b0AC0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "d2f41fb3-defc-4fdf-cf69-a3b34d8a8ffc"
      },
      "source": [
        "!pip install pyspellchecker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/d1/ec4e830e9f9c1fd788e1459dd09279fdf807bc7a475579fd7192450b879c/pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 5.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ_gxY5f0Fwj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b6a10675-6a8d-4513-f7b2-d1a0a5732773"
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "happening\n",
            "{'penning', 'happening', 'henning'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9AlDtsX2058",
        "colab_type": "text"
      },
      "source": [
        "Reference: https://pypi.org/project/pyspellchecker/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXeOxA2vJowM",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: Contraction Mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK3mBeKq3Sn-",
        "colab_type": "text"
      },
      "source": [
        "Contractions are words that we write with an apostrophe. Examples of contractions are words like “ain’t” or “aren’t”. Since we want to standardize our text, it makes sense to expand these contractions. We are going to add a new column to our dataframe called “no_contract” and apply a lambda function to the \"msg\" field which will expand any contractions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfS7b5XQ6zZK",
        "colab_type": "text"
      },
      "source": [
        "First, we install and import the necessary library - contractions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bIyOL3A3sZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "616234e4-cbb6-4e9f-b024-14814add9c50"
      },
      "source": [
        "!pip install contractions\n",
        "import contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/00/92/a05b76a692ac08d470ae5c23873cf1c9a041532f1ee065e74b374f218306/contractions-0.0.25-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 6.9MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 15.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81697 sha256=7ffd2c2e616b50f5716afbf6bc1b383dac0731bc4baab0437f6231612c10ccda\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.25 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEKvXDOH31l7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sms['no_contract'] = sms['msg'].apply(lambda x: [contractions.fix(word) for word in x.split()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRud_Vrk7Ggf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "4be0061c-76b0-42d2-835a-8ff652c6ee33"
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>no_contract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, he...</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                        no_contract\n",
              "0   ham  ...  [Go, until, jurong, point,, crazy.., Available...\n",
              "1   ham  ...             [Ok, lar..., Joking, wif, you, oni...]\n",
              "2  spam  ...  [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
              "3   ham  ...  [you, dun, say, so, early, hor..., you, c, alr...\n",
              "4   ham  ...  [Nah, I, do not, think, he, goes, to, usf,, he...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLGxmOZI7NNn",
        "colab_type": "text"
      },
      "source": [
        "Also, we would want the expanded contractions to be tokenized separately, therefore we convert the lists under the \"no_contract\" column back into strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzTj7U_J7USX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sms[\"msg_str\"] = [' '.join(map(str, l)) for l in sms['no_contract']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO4OPskH7gRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "833eb210-42c4-4d21-8a45-7788ed631445"
      },
      "source": [
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, he...</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                            msg_str\n",
              "0   ham  ...  Go until jurong point, crazy.. Available only ...\n",
              "1   ham  ...                    Ok lar... Joking wif you oni...\n",
              "2  spam  ...  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  ...  you dun say so early hor... you c already then...\n",
              "4   ham  ...  Nah I do not think he goes to usf, he lives ar...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xJy-B1G3ZSC",
        "colab_type": "text"
      },
      "source": [
        "Reference: https://towardsdatascience.com/preprocessing-text-data-using-python-576206753c28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ZrTjxJJqqn",
        "colab_type": "text"
      },
      "source": [
        "### Step 5: Stemming/Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPMcw9E7tpqe",
        "colab_type": "text"
      },
      "source": [
        "The idea of stemming is to reduce different forms of word usage into its root word. For example, “drive”, “drove”, “driving”, “driven”, “driver” are derivatives of the word “drive” and very often researchers want to remove this variability from their corpus. Compared to lemmatization, stemming is certainly the less complicated method but it often does not produce a dictionary-specific morphological root of the word. In other words, stemming the word “pies” will often produce a root of “pi” whereas lemmatization will find the morphological root of “pie”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0jD4XRn_AYM",
        "colab_type": "text"
      },
      "source": [
        "Instead of taking the easy way out with stemming, let’s apply lemmatization to our data but it requires some additional steps compared to stemming. First, we have to apply parts of speech tags, in other words, determine the part of speech (ie. noun, verb, adverb, etc.) for each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gOKB7uD_Zz8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "dbd1d83d-33ab-476d-b6a5-cd61652b2842"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX-fBFBdtqCI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "outputId": "b4beaabf-8a11-4eb3-d507-acebb6bde117"
      },
      "source": [
        "sms['pos_tags'] = sms['no_contract'].apply(nltk.tag.pos_tag)\n",
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>pos_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[(Go, NNP), (until, IN), (jurong, JJ), (point,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[(Free, JJ), (entry, NN), (in, IN), (2, CD), (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[(you, PRP), (dun, VBP), (say, VB), (so, RB), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, he...</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[(Nah, NNP), (I, PRP), (do not, VBP), (think, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                           pos_tags\n",
              "0   ham  ...  [(Go, NNP), (until, IN), (jurong, JJ), (point,...\n",
              "1   ham  ...  [(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...\n",
              "2  spam  ...  [(Free, JJ), (entry, NN), (in, IN), (2, CD), (...\n",
              "3   ham  ...  [(you, PRP), (dun, VBP), (say, VB), (so, RB), ...\n",
              "4   ham  ...  [(Nah, NNP), (I, PRP), (do not, VBP), (think, ...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aP4xl6c_eB8",
        "colab_type": "text"
      },
      "source": [
        "We are going to be using NLTK’s word lemmatizer which needs the parts of speech tags to be converted to wordnet’s format. We’ll write a function which make the proper conversion and then use the function within a list comprehension to apply the conversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE1a4riM_u2X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "486a99fe-af51-42e0-adc7-e40aeea3a107"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQcreCGV_hxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17-r9hWP_o0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "outputId": "3b2aa629-89d7-4bd7-944a-b1dd6e512eb9"
      },
      "source": [
        "sms['wordnet_pos'] = sms['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>wordnet_pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[(Go, NNP), (until, IN), (jurong, JJ), (point,...</td>\n",
              "      <td>[(Go, n), (until, n), (jurong, a), (point,, n)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...</td>\n",
              "      <td>[(Ok, n), (lar..., v), (Joking, n), (wif, n), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[(Free, JJ), (entry, NN), (in, IN), (2, CD), (...</td>\n",
              "      <td>[(Free, a), (entry, n), (in, n), (2, n), (a, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[(you, PRP), (dun, VBP), (say, VB), (so, RB), ...</td>\n",
              "      <td>[(you, n), (dun, v), (say, v), (so, r), (early...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, he...</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[(Nah, NNP), (I, PRP), (do not, VBP), (think, ...</td>\n",
              "      <td>[(Nah, n), (I, n), (do not, v), (think, v), (h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                        wordnet_pos\n",
              "0   ham  ...  [(Go, n), (until, n), (jurong, a), (point,, n)...\n",
              "1   ham  ...  [(Ok, n), (lar..., v), (Joking, n), (wif, n), ...\n",
              "2  spam  ...  [(Free, a), (entry, n), (in, n), (2, n), (a, n...\n",
              "3   ham  ...  [(you, n), (dun, v), (say, v), (so, r), (early...\n",
              "4   ham  ...  [(Nah, n), (I, n), (do not, v), (think, v), (h...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCjEH6jG_iCE",
        "colab_type": "text"
      },
      "source": [
        "Now we can apply NLTK’s word lemmatizer within our trusty list comprehension. Notice, the lemmatizer function requires two parameters the word and its tag (in wordnet form)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96mw3ww8AGNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJtg03P3_iXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "outputId": "3a78d169-0a10-4df1-8c68-ebbccf869299"
      },
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "sms['lemmatized'] = sms['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>wordnet_pos</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[(Go, NNP), (until, IN), (jurong, JJ), (point,...</td>\n",
              "      <td>[(Go, n), (until, n), (jurong, a), (point,, n)...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...</td>\n",
              "      <td>[(Ok, n), (lar..., v), (Joking, n), (wif, n), ...</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[(Free, JJ), (entry, NN), (in, IN), (2, CD), (...</td>\n",
              "      <td>[(Free, a), (entry, n), (in, n), (2, n), (a, n...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[(you, PRP), (dun, VBP), (say, VB), (so, RB), ...</td>\n",
              "      <td>[(you, n), (dun, v), (say, v), (so, r), (early...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, he...</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[(Nah, NNP), (I, PRP), (do not, VBP), (think, ...</td>\n",
              "      <td>[(Nah, n), (I, n), (do not, v), (think, v), (h...</td>\n",
              "      <td>[Nah, I, do not, think, he, go, to, usf,, he, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                         lemmatized\n",
              "0   ham  ...  [Go, until, jurong, point,, crazy.., Available...\n",
              "1   ham  ...             [Ok, lar..., Joking, wif, you, oni...]\n",
              "2  spam  ...  [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
              "3   ham  ...  [you, dun, say, so, early, hor..., you, c, alr...\n",
              "4   ham  ...  [Nah, I, do not, think, he, go, to, usf,, he, ...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKiy3apFJqeb",
        "colab_type": "text"
      },
      "source": [
        "### Step 6: ‘Stop Words’ Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-rMODaGtBlV",
        "colab_type": "text"
      },
      "source": [
        "Some words in the English language, while necessary, don't contribute much to the meaning of a phrase. These words, such as \"when\", \"had\", \"those\" or \"before\", are called stop words and should be filtered out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGzWZxk-8Pil",
        "colab_type": "text"
      },
      "source": [
        "First, we need to import the NLTK stopwords library and set our stopwords to “english”. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IlsjVG-LLWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "63bdae22-72d1-4b78-9089-cfd3a4e907fd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n73ZUUKrtOcP",
        "colab_type": "text"
      },
      "source": [
        "We are going to add a new column “no_stopwords” which will remove the stopwords from the “no_punc” column since it has been tokenized, had been converted to lowercase and punctuation was removed. Once again a for-loop within a lambda function will iterate over the tokens in “no_punc” and only return the tokens which do not exist in our “stop_words” variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMwO1hBltQXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "outputId": "76036838-e452-4bfb-e5ec-7ecd1734e713"
      },
      "source": [
        "sms['stopwords_removed'] = sms['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "sms.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>msg</th>\n",
              "      <th>length</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>no_contract</th>\n",
              "      <th>msg_str</th>\n",
              "      <th>pos_tags</th>\n",
              "      <th>wordnet_pos</th>\n",
              "      <th>lemmatized</th>\n",
              "      <th>stopwords_removed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>111</td>\n",
              "      <td>[Go, until, jurong, point, ,, crazy.., Availab...</td>\n",
              "      <td>[go, until, jurong, point, ,, crazy.., availab...</td>\n",
              "      <td>[go, until, jurong, point, crazy.., available,...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>[(Go, NNP), (until, IN), (jurong, JJ), (point,...</td>\n",
              "      <td>[(Go, n), (until, n), (jurong, a), (point,, n)...</td>\n",
              "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
              "      <td>[go, jurong, point, crazy.., available, bugis,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>29</td>\n",
              "      <td>[Ok, lar, ..., Joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>Ok lar... Joking wif you oni...</td>\n",
              "      <td>[(Ok, NNP), (lar..., VBZ), (Joking, NNP), (wif...</td>\n",
              "      <td>[(Ok, n), (lar..., v), (Joking, n), (wif, n), ...</td>\n",
              "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
              "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>155</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[(Free, JJ), (entry, NN), (in, IN), (2, CD), (...</td>\n",
              "      <td>[(Free, a), (entry, n), (in, n), (2, n), (a, n...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>49</td>\n",
              "      <td>[U, dun, say, so, early, hor, ..., U, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>you dun say so early hor... you c already then...</td>\n",
              "      <td>[(you, PRP), (dun, VBP), (say, VB), (so, RB), ...</td>\n",
              "      <td>[(you, n), (dun, v), (say, v), (so, r), (early...</td>\n",
              "      <td>[you, dun, say, so, early, hor..., you, c, alr...</td>\n",
              "      <td>[u, dun, say, early, hor, ..., u, c, already, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>61</td>\n",
              "      <td>[Nah, I, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
              "      <td>[nah, i, do, n't, think, he, goes, to, usf, he...</td>\n",
              "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
              "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
              "      <td>[(Nah, NNP), (I, PRP), (do not, VBP), (think, ...</td>\n",
              "      <td>[(Nah, n), (I, n), (do not, v), (think, v), (h...</td>\n",
              "      <td>[Nah, I, do not, think, he, go, to, usf,, he, ...</td>\n",
              "      <td>[nah, n't, think, goes, usf, lives, around, th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                  stopwords_removed\n",
              "0   ham  ...  [go, jurong, point, crazy.., available, bugis,...\n",
              "1   ham  ...           [ok, lar, ..., joking, wif, u, oni, ...]\n",
              "2  spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
              "3   ham  ...  [u, dun, say, early, hor, ..., u, c, already, ...\n",
              "4   ham  ...  [nah, n't, think, goes, usf, lives, around, th...\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpqlSX_JASXz",
        "colab_type": "text"
      },
      "source": [
        "Lastly, we should save all of our pre-processing work for the next steps in the workflow. We can simnple save it as a csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRKemvFgAc4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sms.to_csv('sms_spam_collection.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQweWcQorWs6",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "- https://github.com/ujjwalgupta07/spam_ham_detection/blob/master/spam_ham_classification.ipynb\n",
        "- https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
        "- https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python\n",
        "- https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/#d-removing-contractions\n",
        "\n",
        "***\n",
        "- https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
        "- https://inmachineswetrust.com/posts/sms-spam-filter/\n",
        "- https://realpython.com/natural-language-processing-spacy-python/\n",
        "- https://github.com/WomenWhoCode/WWCodeDataScience/blob/master/Intro_to_MachineLearning/1_Introduction.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3_uGCKBXV6z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}